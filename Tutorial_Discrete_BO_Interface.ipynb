{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8929130",
   "metadata": {},
   "source": [
    "## Use our trained models\n",
    "The models used for our final setup can be found in `pfns4bo/final_models` or directly as\n",
    "- `pfns4bo.hebo_plus_model`\n",
    "- `pfns4bo.bnn_model`\n",
    "- `pfns4bo.hebo_plus_userprior_model`\n",
    "\n",
    "### Use on discrete benchmarks\n",
    "To use our model on discrete benchmarks, we recommend using our HPO-B interface `pfns4bo.scripts.acquisition_functions.TransformerBOMethod`.\n",
    "\n",
    "We called this interface like this for the eval's on HPO-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pfns4bo\n",
    "from pfns4bo.scripts.acquisition_functions import TransformerBOMethod\n",
    "from pfns4bo.scripts.tune_input_warping import fit_input_warping\n",
    "\n",
    "device = 'cpu:0'\n",
    "\n",
    "# For HEBO+ \n",
    "model_name = pfns4bo.hebo_plus_model\n",
    "# For BNN\n",
    "#model_name = pfns4bo.bnn_model\n",
    "\n",
    "# for correctly specified search spaces (e.g. correctly applied log transformations)\n",
    "pfn_bo = TransformerBOMethod(torch.load(model_name), device=device)\n",
    "\n",
    "# for mis-specified search spaces\n",
    "#pfn_bo = TransformerBOMethod(torch.load(folder_with_models + model_name), fit_encoder=fit_input_warping, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa3f78",
   "metadata": {},
   "source": [
    "The interface expects all features to be normalized to a [0,1] range and all features have to be scalars/floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obs = torch.rand(10,1) # n_examples x n_feats\n",
    "y_obs = X_obs[:,0]*.2 + torch.randn_like(X_obs[:,0]) * .01 # n_examples\n",
    "X_pen = torch.linspace(0,1,1000)[:,None] # n_pending x n_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d996890",
   "metadata": {},
   "source": [
    "Now that we have data, we can calculate our acquisition like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_idx, eis = pfn_bo.observe_and_suggest(X_obs=X_obs, y_obs=y_obs, X_pen=X_pen, return_actual_ei=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(X_pen, eis, label='EI')\n",
    "\n",
    "ax1.set_ylabel('acquisition value')\n",
    "\n",
    "ax1.legend(loc=0)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.scatter(X_obs, y_obs, label='Observations')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "\n",
    "ax2.legend(loc=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b00b8",
   "metadata": {},
   "source": [
    "### We can use another acquisition function\n",
    "Just specify `acq_function='pi'` or `acq_function='ucb'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn_bo_pi = TransformerBOMethod(torch.load(model_name), device=device, acq_function='pi')\n",
    "rec_idx_pi, pis = pfn_bo_pi.observe_and_suggest(X_obs=X_obs, y_obs=y_obs, X_pen=X_pen, return_actual_ei=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(X_pen, eis, label='EI')\n",
    "ax1.plot(X_pen, pis, label='PI')\n",
    "\n",
    "ax1.set_ylabel('acquisition value')\n",
    "\n",
    "ax1.legend(loc=0)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.scatter(X_obs, y_obs, label='Observations')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "\n",
    "ax2.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62bf2ed",
   "metadata": {},
   "source": [
    "You can even set `acq_function='mean'` to get the mean, by default we use a power transforms on y's though, so it might look a little different than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn_bo_mean = TransformerBOMethod(torch.load(model_name), device=device, acq_function='mean', apply_power_transform=False)\n",
    "rec_idx_mean, means = pfn_bo_mean.observe_and_suggest(X_obs=X_obs, y_obs=y_obs, X_pen=X_pen, return_actual_ei=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X_obs, y_obs, label='Observations')\n",
    "plt.plot(X_pen, means, label='Predicted Means')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985812b",
   "metadata": {},
   "source": [
    "### Use user priors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the order of hps in our benchmark is 'lr_decay_factor', 'lr_initial', 'lr_power', 'opt_momentum', 'epoch', 'activation'\n",
    "pfn_bo_w_userprior = TransformerBOMethod(torch.load(pfns4bo.hebo_plus_userprior_model), style=\\\n",
    "                     torch.tensor([\n",
    "                         .9, 0/4, 1/4, # feature 1 has .9 prob is given to the prior where all max's lie in [0,.25], (1-.9) to be anywhere in [0,1]\n",
    "                     ]).view(1,-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_idx_w_userprior, eis_w_userprior = pfn_bo_w_userprior.observe_and_suggest(X_obs=X_obs, y_obs=y_obs, X_pen=X_pen, return_actual_ei=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(X_pen, eis, label='EI')\n",
    "ax1.plot(X_pen, eis_w_userprior, label='EI w/ user prior')\n",
    "ax1.set_ylabel('acquisition value')\n",
    "\n",
    "ax1.legend(loc=0)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.scatter(X_obs, y_obs, label='Observations')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "\n",
    "ax2.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25d488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
